{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural_Language_Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW2fydwXLkQi",
        "outputId": "5181fe2c-8529-44bc-ebec-b4825099496a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r0% [2 InRelease gpgv 1,575 B] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [2 InRelease gpgv 1,575 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:13 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,957 kB]\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,004 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,498 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,168 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,734 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n",
            "Fetched 12.9 MB in 6s (2,005 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.2.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Tokens\").getOrCreate()"
      ],
      "metadata": {
        "id": "ZKeDh6PQMMcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Tokenizer Library\n",
        "from pyspark.ml.feature import Tokenizer"
      ],
      "metadata": {
        "id": "FWU6-MTKMV7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample DataFrame\n",
        "dataframe = spark.createDataFrame([\n",
        "                                   (0, \"Spark has not been working today\"),\n",
        "                                   (1, \"What am I doing wrong\"),\n",
        "                                   (2, \"I need help\")\n",
        "],[\"id\", \"sentence\"])\n",
        "dataframe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDiagBNtMavE",
        "outputId": "3099ba9f-0eed-4034-dc6e-8cf8cb6fd27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|            sentence|\n",
            "+---+--------------------+\n",
            "|  0|Spark has not bee...|\n",
            "|  1|What am I doing w...|\n",
            "|  2|         I need help|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize sentences with Spark\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC7UeXAjM0TA",
        "outputId": "82726957-792c-4a6c-bc7f-e12762520ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer_cfcb4538efc0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform and show DataFrame\n",
        "tokenized_df = tokenizer.transform(dataframe)\n",
        "tokenized_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvHdl06uNJrP",
        "outputId": "ff2d4ddd-022e-4ce5-9cb6-b3c0f796eeaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+---------------------------------------+\n",
            "|id |sentence                        |words                                  |\n",
            "+---+--------------------------------+---------------------------------------+\n",
            "|0  |Spark has not been working today|[spark, has, not, been, working, today]|\n",
            "|1  |What am I doing wrong           |[what, am, i, doing, wrong]            |\n",
            "|2  |I need help                     |[i, need, help]                        |\n",
            "+---+--------------------------------+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will turn this into a 'user-defined function' (UDF) below \n",
        "# function returns the length of a list\n",
        "def word_list_length(word_list):\n",
        "  return len(word_list)"
      ],
      "metadata": {
        "id": "nCFpkaJzNgkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the udf function, the col function (to select a column to be passed into the udf), and the type IntegerType that will be used in our udf to define the datatype\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "metadata": {
        "id": "Si0_sNeANxi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our function into a UDF\n",
        "count_tokens = udf(word_list_length, IntegerType())"
      ],
      "metadata": {
        "id": "M9595s-jOLpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create our Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "\n",
        "# Transform DataFrame\n",
        "tokenized_df = tokenizer.transform(dataframe)\n",
        "\n",
        "# Select the needed columns and don't truncate results\n",
        "tokenized_df.withColumn(\"tokens\", count_tokens(col(\"words\"))).show(truncate=False)"
      ],
      "metadata": {
        "id": "Gs6vIwHfOn2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98b83c3-6043-42ae-cc9c-4901babd1b04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+---------------------------------------+------+\n",
            "|id |sentence                        |words                                  |tokens|\n",
            "+---+--------------------------------+---------------------------------------+------+\n",
            "|0  |Spark has not been working today|[spark, has, not, been, working, today]|6     |\n",
            "|1  |What am I doing wrong           |[what, am, i, doing, wrong]            |5     |\n",
            "|2  |I need help                     |[i, need, help]                        |3     |\n",
            "+---+--------------------------------+---------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Stop Words library\n",
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "metadata": {
        "id": "SMJ1aiNMP3MQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the remover\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")"
      ],
      "metadata": {
        "id": "nerg7AoBRMPf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform and show data\n",
        "remover.transform(tokenized_df).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5Y4o4kFRaNz",
        "outputId": "f47e9f1f-317c-429d-ebc0-2cc5f8dc3ed4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+---------------------------------------+-----------------------+\n",
            "|id |sentence                        |words                                  |filtered               |\n",
            "+---+--------------------------------+---------------------------------------+-----------------------+\n",
            "|0  |Spark has not been working today|[spark, has, not, been, working, today]|[spark, working, today]|\n",
            "|1  |What am I doing wrong           |[what, am, i, doing, wrong]            |[wrong]                |\n",
            "|2  |I need help                     |[i, need, help]                        |[need, help]           |\n",
            "+---+--------------------------------+---------------------------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OtEbGyeGRkQY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}